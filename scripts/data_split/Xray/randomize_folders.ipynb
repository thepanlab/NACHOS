{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import csv\n",
    "from itertools import cycle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_filename(old_filename, datasets, new_dir_name):\n",
    "    \"\"\"\n",
    "    Rename the file by replacing the dataset name with the directory name.\n",
    "\n",
    "    :pram old_filename (str): The original filename.\n",
    "    :pram datasets (list[str]): List of dataset names.\n",
    "    :pram new_dir_name (str): Name of the new directory.\n",
    "    :return (str): The renamed file.\n",
    "    \"\"\"\n",
    "    for dataset in datasets:\n",
    "        if dataset in old_filename:\n",
    "            return old_filename.replace(dataset, new_dir_name)\n",
    "    return old_filename\n",
    "\n",
    "\n",
    "def create_new_directories(base_new_dir, class_structure):\n",
    "    \"\"\"\n",
    "    Create the required new directories if they don't exist.\n",
    "\n",
    "    :pram base_new_dir (str): The base directory where new directories should be created.\n",
    "    :pram class_structure (str): path structure for class-specific directories.\n",
    "    :return: list[str]: List of new directory paths.\n",
    "    \"\"\"\n",
    "    directory_names = [\"One\", \"Two\", \"Three\", \"Four\"]\n",
    "    new_dirs = [os.path.join(base_new_dir, name, class_structure) for name in directory_names]\n",
    "    for new_dir in new_dirs:\n",
    "        os.makedirs(new_dir, exist_ok=True)\n",
    "    return new_dirs\n",
    "\n",
    "\n",
    "def collect_image_sources(base_directory, datasets, class_structure):\n",
    "    \"\"\"\n",
    "    Collect all image paths from the source directories.\n",
    "\n",
    "    :pram base_directory (str): Base directory to search from.\n",
    "    :pram datasets (list[str]): List of dataset names.\n",
    "    :pram class_structure (str): path structure for class-specific directories.\n",
    "    :return: list[str]: List of all image source paths.\n",
    "    \"\"\"\n",
    "    all_image_sources = []\n",
    "    for dataset in datasets:\n",
    "        dir_path = os.path.join(base_directory, dataset, class_structure)\n",
    "        image_sources = [os.path.join(dir_path, img_name) for img_name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, img_name))]\n",
    "        all_image_sources.extend(image_sources)\n",
    "        print(f\"For {dataset}, {class_structure.split('/')[0]}, {class_structure.split('/')[1]}: {len(image_sources)} files.\")\n",
    "    return all_image_sources\n",
    "\n",
    "\n",
    "def distribute_files_and_log(all_image_sources, datasets, new_dirs):\n",
    "    \"\"\"\n",
    "    Distribute files across the new directories and log the changes.\n",
    "\n",
    "    :pram all_image_sources (list[str]): List of all image source paths.\n",
    "    :pram datasets (list[str]): List of dataset names.\n",
    "    :pram new_dirs (list[str]): List of destination directories.\n",
    "    \"\"\"\n",
    "    logs = []\n",
    "\n",
    "    #seed before shuffle random.seed\n",
    "    random.shuffle(all_image_sources)\n",
    "\n",
    "    num_files = len(all_image_sources)\n",
    "    files_per_directory = num_files // 4\n",
    "    leftover_files = num_files % 4\n",
    "    index = 0\n",
    "\n",
    "    directory_names = [\"One\", \"Two\", \"Three\", \"Four\"]\n",
    "\n",
    "    for dir_num, new_dir in enumerate(new_dirs):\n",
    "        limit = files_per_directory\n",
    "        if dir_num < leftover_files:\n",
    "            limit += 1\n",
    "\n",
    "        for _ in range(limit):\n",
    "            img_path = all_image_sources[index]\n",
    "            old_filename = os.path.basename(img_path)\n",
    "            new_filename = rename_filename(old_filename, datasets, directory_names[dir_num])\n",
    "            new_path = os.path.join(new_dir, new_filename)\n",
    "            shutil.copy2(img_path, new_path)\n",
    "            logs.append((img_path, os.path.dirname(img_path), new_dir))\n",
    "            index += 1\n",
    "\n",
    "        print(f\"{new_dir}: {len(os.listdir(new_dir))} files.\")\n",
    "\n",
    "    # Logging\n",
    "    base_new_dir = os.path.dirname(os.path.dirname(new_dirs[0]))\n",
    "    csv_dir = os.path.join(\"/ssd2/pipeline/\", \"csv/file_log/split1\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    class_structure = os.path.relpath(new_dirs[0], base_new_dir)\n",
    "    log_file_path = os.path.join(csv_dir, f\"file_log_{class_structure.replace('/', '_')}.csv\")\n",
    "    with open(log_file_path, \"w\", newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Original File\", \"Original Directory\", \"New Directory\"])\n",
    "        writer.writerows(logs)\n",
    "\n",
    "\n",
    "def random_split(base_directory, class_label):\n",
    "    \"\"\"\n",
    "    Randomly splits and copies files from source directories of a specific class\n",
    "    to 4 new directories, maintaining their class structure.\n",
    "\n",
    "    :pram base_directory (str): Base directory containing datasets.\n",
    "    :pram class_label (tuple[str, str]): Tuple of class labels.\n",
    "    \"\"\"\n",
    "    datasets = [\"NIH\", \"CheXpert\", \"MIMIC\", \"PadChest\"]\n",
    "    base_new_dir = \"/ssd2/pipeline/datasets/620_file_size/updated/split1_random\"\n",
    "    class_structure = os.path.join(*class_label)\n",
    "    new_dirs = create_new_directories(base_new_dir, class_structure)\n",
    "    all_image_sources = collect_image_sources(base_directory, datasets, class_structure)\n",
    "    distribute_files_and_log(all_image_sources, datasets, new_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NIH, Cardiomegaly, PA: 620 files.\n",
      "For CheXpert, Cardiomegaly, PA: 620 files.\n",
      "For MIMIC, Cardiomegaly, PA: 620 files.\n",
      "For PadChest, Cardiomegaly, PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/One/Cardiomegaly/PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/Two/Cardiomegaly/PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/Three/Cardiomegaly/PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/Four/Cardiomegaly/PA: 620 files.\n",
      "For NIH, No-Finding, PA: 620 files.\n",
      "For CheXpert, No-Finding, PA: 620 files.\n",
      "For MIMIC, No-Finding, PA: 620 files.\n",
      "For PadChest, No-Finding, PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/One/No-Finding/PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/Two/No-Finding/PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/Three/No-Finding/PA: 620 files.\n",
      "/ssd2/pipeline/datasets/620_file_size/updated/split1_random/Four/No-Finding/PA: 620 files.\n"
     ]
    }
   ],
   "source": [
    "base_directory = \"/ssd2/pipeline/datasets/620_file_size/split3_dataset\"\n",
    "class_labels = [\n",
    "    (\"Cardiomegaly\", \"PA\"),\n",
    "    (\"No-Finding\", \"PA\"),\n",
    "]\n",
    "\n",
    "# Using random_split2_patient\n",
    "for class_label in class_labels:\n",
    "    random_split(base_directory, class_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For NIH, Cardiomegaly, PA: 620 files.\n",
      "For CheXpert, Cardiomegaly, PA: 620 files.\n",
      "For MIMIC, Cardiomegaly, PA: 620 files.\n",
      "For PadChest, Cardiomegaly, PA: 620 files.\n",
      "For NIH, No-Finding, PA: 620 files.\n",
      "For CheXpert, No-Finding, PA: 620 files.\n",
      "For MIMIC, No-Finding, PA: 620 files.\n",
      "For PadChest, No-Finding, PA: 620 files.\n"
     ]
    }
   ],
   "source": [
    "def extract_patientinfo(filename):\n",
    "    \"\"\"\n",
    "    Extracts the patient info from the filename\n",
    "    \n",
    "    :param filename: The filename from which to extract the patient info \n",
    "    :return: The extracted patient info.\n",
    "    \"\"\"\n",
    "    # Example filename: \"112559_MIMIC_Cardiomegaly_PA_19598446.png\"\n",
    "    parts = filename.split('_')\n",
    "    return parts[-1].split('.')[0], parts[2], parts[3] \n",
    "\n",
    "\n",
    "def collect_image_sources(base_directory, datasets, class_structures):\n",
    "    \"\"\"\n",
    "    Collect all image paths from the source directories.\n",
    "\n",
    "    :param base_directory (str): Base directory to search from.\n",
    "    :param datasets (list[str]): List of dataset names.\n",
    "    :param class_structures (list[str]): List of path structures for class-specific directories.\n",
    "    :return: list[str]: List of all image source paths.\n",
    "    \"\"\"\n",
    "    all_image_sources = []\n",
    "    for class_structure in class_structures:\n",
    "        for dataset in datasets:\n",
    "            dir_path = os.path.join(base_directory, dataset, class_structure)\n",
    "            image_sources = [os.path.join(dir_path, img_name) for img_name in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, img_name))]\n",
    "            all_image_sources.extend(image_sources)\n",
    "            print(f\"For {dataset}, {class_structure.split(os.path.sep)[0]}, {class_structure.split(os.path.sep)[1]}: {len(image_sources)} files.\")\n",
    "    return all_image_sources\n",
    "\n",
    "def create_new_directories(base_new_dir, class_structures):\n",
    "    \"\"\"\n",
    "    Create the required new directories for each class structure within each fold.\n",
    "\n",
    "    :param base_new_dir (str): The base directory where new directories should be created.\n",
    "    :param class_structures (list[str]): List of path structures for class-specific directories.\n",
    "    :return: list[str]: List of new directory paths.\n",
    "    \"\"\"\n",
    "    directory_names = [\"One\", \"Two\", \"Three\", \"Four\"]\n",
    "    new_dirs = []\n",
    "    for directory_name in directory_names:\n",
    "        for class_structure in class_structures:\n",
    "            new_dir = os.path.join(base_new_dir, directory_name, class_structure)\n",
    "            os.makedirs(new_dir, exist_ok=True)\n",
    "            new_dirs.append(new_dir)\n",
    "    return new_dirs\n",
    "\n",
    "def get_directory_for_image(base_new_dir, dest_dir_name, patient_class, patient_view):\n",
    "    \"\"\"\n",
    "    Construct the complete directory path for storing the image.\n",
    "\n",
    "    :param base_new_dir (list): List of pre-created directory paths.\n",
    "    :param dest_dir_name (str): Base directory name like \"One\", \"Two\", etc.\n",
    "    :param patient_class (str): The class of the patient (e.g., 'Cardiomegaly').\n",
    "    :param patient_view (str): The view of the image (e.g., 'PA').\n",
    "    :return: str: The complete directory path.\n",
    "    \"\"\"\n",
    "    directory_suffix = f\"{dest_dir_name}/{patient_class}/{patient_view}\"\n",
    "    for dir_path in base_new_dir:\n",
    "        if directory_suffix in dir_path:\n",
    "            return dir_path\n",
    "    raise ValueError(\"No matching directory found for the given base name, class, and view.\")\n",
    "\n",
    "\n",
    "\n",
    "def distribute_files_and_log_patient(all_image_sources, datasets, base_new_dir):\n",
    "    patientid_to_paths = defaultdict(lambda: {'Paths': [], 'Class': [], 'View': []})\n",
    "    for img_path in all_image_sources:\n",
    "        patientinfo = extract_patientinfo(os.path.basename(img_path))\n",
    "        patient_id = patientinfo[0]\n",
    "        patientid_to_paths[patient_id]['Paths'].append(img_path)\n",
    "        patientid_to_paths[patient_id]['Class'].append(patientinfo[1])\n",
    "        patientid_to_paths[patient_id]['View'].append(patientinfo[2])\n",
    "\n",
    "    directory_patient_map = {}\n",
    "    directory_names = [\"One\", \"Two\", \"Three\", \"Four\"]\n",
    "    dir_file_counts = {dir_name: 0 for dir_name in directory_names}\n",
    "    logs_by_class = defaultdict(list)\n",
    "\n",
    "    for patient_id, info in patientid_to_paths.items():\n",
    "        if patient_id not in directory_patient_map:\n",
    "            selected_dir = min(dir_file_counts, key=lambda k: dir_file_counts[k])\n",
    "            directory_patient_map[patient_id] = selected_dir\n",
    "            dir_file_counts[selected_dir] += len(info['Paths'])\n",
    "\n",
    "        for idx, img_path in enumerate(info['Paths']):\n",
    "            class_label = info['Class'][idx]\n",
    "            view_label = info['View'][idx]\n",
    "            # Determine the destination directory for each image based on its class\n",
    "            dest_dir_base = get_directory_for_image(base_new_dir, directory_patient_map[patient_id], class_label, view_label)\n",
    "            os.makedirs(dest_dir_base, exist_ok=True)\n",
    "\n",
    "            old_filename = os.path.basename(img_path)\n",
    "            new_filename = rename_filename(old_filename, datasets, directory_patient_map[patient_id])\n",
    "            new_dir = os.path.join(dest_dir_base, new_filename)\n",
    "            shutil.copy2(img_path, new_dir)\n",
    "            logs_by_class[class_label].append((img_path, os.path.dirname(img_path), new_dir))\n",
    "\n",
    "    # Logging separated by class\n",
    "    for class_label, logs in logs_by_class.items():\n",
    "        csv_dir = os.path.join(\"/ssd2/pipeline/\", \"csv/file_log/split2\")\n",
    "        os.makedirs(csv_dir, exist_ok=True)\n",
    "        log_file_path = os.path.join(csv_dir, f\"file_log_{class_label}_{view_label}.csv\")\n",
    "        with open(log_file_path, \"w\", newline='') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([\"Original File\", \"Original Directory\", \"New Directory\"])\n",
    "            writer.writerows(logs)\n",
    "\n",
    "\n",
    "def random_split2(base_directory, class_labels):\n",
    "    datasets = [\"NIH\", \"CheXpert\", \"MIMIC\", \"PadChest\"]\n",
    "    base_new_dir = \"/ssd2/pipeline/datasets/620_file_size/updated/split2_patient\"\n",
    "\n",
    "    # Create a list of class structures\n",
    "    class_structures = [os.path.join(class_label[0], class_label[1]) for class_label in class_labels]\n",
    "    new_dirs = create_new_directories(base_new_dir, class_structures)\n",
    "\n",
    "    all_image_sources = collect_image_sources(base_directory, datasets, class_structures)\n",
    "    distribute_files_and_log_patient(all_image_sources,datasets, new_dirs)\n",
    "\n",
    "# Example of how to call the function with class labels\n",
    "class_labels = [\n",
    "    (\"Cardiomegaly\", \"PA\"),\n",
    "    (\"No-Finding\", \"PA\"),\n",
    "]\n",
    "random_split2(\"/ssd2/pipeline/datasets/620_file_size/split3_dataset\", class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv_and_analyze(*file_paths):\n",
    "    # Load and concatenate CSV files\n",
    "    data_frames = [pd.read_csv(path) for path in file_paths]\n",
    "    data = pd.concat(data_frames, ignore_index=True)\n",
    "    \n",
    "    # Information extraction\n",
    "    data['ID'] = data['Original File'].apply(lambda x: x.split('_')[-1].split('.')[0])\n",
    "    data['Dataset'] = data['Original File'].apply(lambda x: x.split('/')[6])\n",
    "    data['Class'] = data['Original File'].apply(lambda x: x.split('/')[7])\n",
    "    data['New Folder Name'] = data['New Directory'].apply(lambda x: x.split('/split2_patient/')[1].split('/')[0])\n",
    "    data['New ID'] = data['ID'].astype(str) + \"_\" + data['Dataset'].astype(str)\n",
    "\n",
    "    \n",
    "    # Count total number of images\n",
    "    total_images = data.shape[0]\n",
    "\n",
    "    datasubset = data[['New ID','ID','Class', 'Dataset', 'New Folder Name']]\n",
    "    # Count total number of Patient Counts\n",
    "    total_unique_ids = data['New ID'].nunique()\n",
    "    \n",
    "    # Calculate the number of IDs that appear more than once\n",
    "    id_counts = data['New ID'].value_counts()\n",
    "    non_unique_ids = id_counts[id_counts > 1]\n",
    "    \n",
    "    # Calculate the total instances of IDs that are not unique\n",
    "    count_of_non_unique_ids = non_unique_ids.count()\n",
    "    \n",
    "    \n",
    "     # Count Patient Counts by various groupings\n",
    "    ids_by_class = data.groupby(['Class'])['New ID'].nunique().reset_index(name='Patient Counts')\n",
    "    ids_by_dataset = data.groupby(['Dataset'])['New ID'].nunique().reset_index(name='Patient Counts')\n",
    "    ids_by_folder = data.groupby(['New Folder Name'])['New ID'].nunique().reset_index(name='Patient Counts')\n",
    "    ids_by_class_dataset = data.groupby(['Class','Dataset'])['New ID'].nunique().reset_index(name='Patient Counts')\n",
    "    ids_by_class_dataset_folder = data.groupby(['Class', 'Dataset', 'New Folder Name'])['New ID'].nunique().reset_index(name='Patient Counts')\n",
    "    \n",
    "    \n",
    "    # Analysis for each dataset in each folder\n",
    "    dataset_in_folder_analysis = data.groupby(['New Folder Name', 'Dataset', 'Class']).size().reset_index(name='Image Count')\n",
    "    \n",
    "\n",
    "    # Filter for 'Cardiomegaly' and 'No-Finding' and re-calculate the counts\n",
    "    filtered_data = dataset_in_folder_analysis[dataset_in_folder_analysis['Class'].isin(['Cardiomegaly', 'No-Finding'])]\n",
    "    \n",
    "    # Create a pivot table for the new analysis\n",
    "    dataset_and_class_in_folder_analysis = filtered_data.pivot_table(\n",
    "        index=['New Folder Name', 'Dataset'],\n",
    "        columns='Class',\n",
    "        values='Image Count',\n",
    "        fill_value=0,  # Fill missing values with 0\n",
    "        aggfunc='sum'  # Ensure correct aggregation\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Flatten the column hierarchy for the new analysis DataFrame\n",
    "    dataset_and_class_in_folder_analysis.columns = ['New Folder Name', 'Dataset', 'Cardiomegaly', 'No-Finding']\n",
    "    \n",
    "    \n",
    "    # Define output directory and ensure it exists\n",
    "    output_directory = f'/ssd/averijordan/csv/split2_patient/'\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    class_data_folder = f'{output_directory}dataset_and_class_in_folder_analysis.csv'\n",
    "    ids_by_class.to_csv(f'{output_directory}ids_by_class.csv', index=False)\n",
    "    ids_by_dataset.to_csv(f'{output_directory}ids_by_dataset.csv', index=False)\n",
    "    ids_by_folder.to_csv(f'{output_directory}ids_by_folder.csv', index=False)\n",
    "    ids_by_class_dataset.to_csv(f'{output_directory}ids_by_class_dataset.csv', index=False)\n",
    "    ids_by_class_dataset_folder.to_csv(f'{output_directory}ids_by_class_dataset_folder.csv', index=False)\n",
    "    datasubset.to_csv(f'{output_directory}datasubset.csv', index=False)\n",
    "\n",
    "    \n",
    "    dataset_and_class_in_folder_analysis.to_csv(class_data_folder, index=False)\n",
    "    \n",
    "    # Return paths to both CSV files along with other stats\n",
    "    return {\n",
    "        'Total Images': total_images,\n",
    "        'Total Patient Counts': total_unique_ids,\n",
    "        'Count of Non-Patient Counts': count_of_non_unique_ids,\n",
    "        'New Dataset and Class in Folder Analysis CSV File Path': class_data_folder,\n",
    "        'CSV File Path for IDs by Class': f'{output_directory}ids_by_class.csv',\n",
    "        'CSV File Path for IDs by Dataset': f'{output_directory}ids_by_dataset.csv',\n",
    "        'CSV File Path for IDs by Folder': f'{output_directory}ids_by_folder.csv',\n",
    "        'CSV File Path for IDs by Class, Dataset, and Folder': f'{output_directory}ids_by_class_dataset_folder.csv'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "'/ssd2/pipeline/csv/file_log/split2/file_log_Cardiomegaly_PA.csv',\n",
    "'/ssd2/pipeline/csv/file_log/split2/file_log_No-Finding_PA.csv'\n",
    "]\n",
    "results = load_csv_and_analyze(*file_paths)\n",
    "print(\"Overall Results:\")\n",
    "print(\"Total number of images:\", results['Total Images'])\n",
    "print(\"Total number of Patient Counts:\", results['Total Patient Counts'])\n",
    "print(\"Number of IDs that appear more than once:\", results['Count of Non-Patient Counts'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "'/ssd2/pipeline/csv/file_log/split1/file_log_Cardiomegaly_PA.csv',\n",
    "'/ssd2/pipeline/csv/file_log/split1/file_log_No-Finding_PA.csv'\n",
    "]\n",
    "results = load_csv_and_analyze(*file_paths)\n",
    "print(\"Overall Results:\")\n",
    "print(\"Total number of images:\", results['Total Images'])\n",
    "print(\"Total number of unique IDs:\", results['Total Unique IDs'])\n",
    "print(\"Number of IDs that appear more than once:\", results['Count of Non-Unique IDs'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
